% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_links_pagesource_html.R
\name{get_links}
\alias{get_links}
\title{Automatically collect specific links provided on a website.}
\usage{
get_links(website, start_website, page_ex, links_xpath, n, time_out = 1)
}
\arguments{
\item{website}{The target website with the links to the desired content.}

\item{start_website}{The main webpage of the case of interest.}

\item{page_ex}{The page counter in the link of the main website which - most likely - refers to the links with content on several pages.
Typically an expression such as "page/30" at the end of the link.}

\item{links_xpath}{The general xpath node which contains the links of interest, often similar to "//h2//a".}

\item{n}{The number of pages on the target website which contains the links of interest.}

\item{time_out}{By default 1 sec, simulates human action and prevents overloading servers (too many requests in too little time)}
}
\value{
A character vector with all links ("all_links)
}
\description{
A function to automatically collect specific links on a website, e.g. to public speeches on official websites.
}
\examples{
#Don't run
#Get the links provided on a website
#get_links()

}
